<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<!-- CUSTOMIZE THIS! -->
<title>Riga 2019</title>
<meta name="author" content="Christof Schöch">
<!-- END -->
<meta name="description" content="Slides">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
<link rel="stylesheet" href="css/reveal.css">
<link rel="stylesheet" href="css/theme/simple.css" id="theme">
<!-- Code syntax highlighting -->
<link rel="stylesheet" href="lib/css/zenburn.css">
<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>
<!--[if lt IE 9]>
<script src="lib/js/html5shiv.js"></script>
<![endif]-->
</head>

<body>
<div class="reveal">
<div class="slides">
<section data-markdown="" data-separator="^\n---\n" data-separator-vertical="^\n--\n" data-charset="utf-8">
<script type="text/template">

<!-- THIS IS WHERE THE CONTENT GOES! -->
<!-- Any section element inside of this container is displayed as a slide -->


## Distributional Semantics and Topic Modeling: Theory and Application
<br/>
<hr/>
<br/>
<br/>
<p><b><a href="http://www.digitalhumanities.lv/bssdh/2019/">Baltic Summer School of Digital Humanities: <br/>Essentials of Coding and Encoding
</a></b><br/>Riga, July 2019</p>
<br/>
<hr/>
<p>Christof Schöch<br/>(Trier Center for Digital Humanities, Trier, Germany)</p>
<img height="50" data-src="img/basics/uni-trier.png"></img>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img height="50" data-src="img/basics/tcdh.png"></img></p>


---
## Overview
<br/>
1. [Introduction](#/2) 
1. [Distributional Semantics: Principles and Methods](#/3) 
2. [What are Word Embeddings?](#/4) 
2. [What is Topic Modeling? Examples](#/5) 
3. [Topic Models: the Theory](#/6)
3. [A Topic Modeling pipeline](#/7)
4. [First steps doing Topic Modeling](#/8)
5. [Advanced issues in Topic Modeling](#/9)
5. [Wrapping up](#/10)



---
# Introduction

--
## About this workshop 
<br/>
* Context, examples, theory, demo, hands-on <!-- .element: class="fragment" data-fragment-index="1" -->
* Python-based, but not a Python workshop<br/>("read and run" rather than write code) <!-- .element: class="fragment" data-fragment-index="2" -->
* Learning goal: you understand how a Topic Model is created and can run your own Topic Modeling Pipeline <!-- .element: class="fragment" data-fragment-index="3" -->

<br/>
<br/>
<hr>
* Slides available online: https://christofs.github.io/riga/#/ 
* Download code and sample datasets: <br/>https://github.com/dh-trier/topicmodeling 

--
## Introductions to Python
<br/>
* A pretty motivating and extensive video tutorial: Mosh, Python Tutorial for Beginners: https://www.youtube.com/watch?v=_uQrJ0TkZlc
* hands-on, interactive tutorial: Folgert Karstorp, Python Programming for the Humanities, https://www.karsdorp.io/python-course/
* A useful introductory book: Robert Downey, Think Python, 2nd edition: https://greenteapress.com/wp/think-python-2e/ 

--
## About myself 
<br/>
* Professor of Digital Humanities
* Not a computer scientist, not a statistician
* French literary scholar by training
* Interests in corpus building and quantitative text analysis
* see: https://christof-schoech.de/en 
 

--
## About you: raise your hand if...
<br/>
* ... you are a literary scholar <!-- .element: class="fragment" data-fragment-index="1" -->
* ... you are a historian <!-- .element: class="fragment" data-fragment-index="2" -->
* ... you are a sociologist <!-- .element: class="fragment" data-fragment-index="3" -->
* ... you are a (computational / corpus) linguist <!-- .element: class="fragment" data-fragment-index="4" -->
* ... you are a computer scientist <!-- .element: class="fragment" data-fragment-index="5" -->
* ... you are a digital humanist <!-- .element: class="fragment" data-fragment-index="6" -->
* ... you are a librarian <!-- .element: class="fragment" data-fragment-index="7" -->
* ... you consider yourself to be a local <!-- .element: class="fragment" data-fragment-index="8" -->


---
# Distributional Semantics: Principles and Methods

--
## Basic intuition about distributional semantics
<br/>
* "Her friend's ...... was located on the second floor of the house." 
<br/><br/>
* "apartment" ! <!-- .element: class="fragment" data-fragment-index="1" -->
* "room" ! <!-- .element: class="fragment" data-fragment-index="2" -->
* "balcony" ? <!-- .element: class="fragment" data-fragment-index="3" -->
* "cat" ?? <!-- .element: class="fragment" data-fragment-index="4" -->
* "shark" ??? <!-- .element: class="fragment" data-fragment-index="5" -->


--
## What does this example tell us?
<br/>
* We are able to rank the likelihood of these words in the given context <!-- .element: class="fragment" data-fragment-index="1" -->
* We use world knowledge, but also linguistic competency, for this <!-- .element: class="fragment" data-fragment-index="2" -->
* Computers can learn this too, based on cooccurrence patterns <!-- .element: class="fragment" data-fragment-index="3" -->
* That's how distributional semantics works! <!-- .element: class="fragment" data-fragment-index="4" -->


--
## Basic idea
<br/>
* The meaning of words depends on their context<br/> "You shall know a word by the company it keeps" (Firth, 1957) <!-- .element: class="fragment" data-fragment-index="1" -->
* Words frequently appearing in similar contexts have similar meanings <!-- .element: class="fragment" data-fragment-index="2" -->
* Words that can appear in very similar, specific contexts have similar grammatical functions <!-- .element: class="fragment" data-fragment-index="3" -->

--
## Two applications of this idea
<br/>
* Topic Modeling
* Word Embeddings

---
# What are Word Embeddings? 

--
## Information Retrieval: Vector Space Model
<br/>
* Each document has a certain place in a vector space <!-- .element: class="fragment" data-fragment-index="1" -->
* That place is determined by the keywords that appear in the document <!-- .element: class="fragment" data-fragment-index="2" -->
* Each word is a dimension in the vector space <!-- .element: class="fragment" data-fragment-index="3" -->
* Documents with shared vocabulary end up in the same area of the vector space <!-- .element: class="fragment" data-fragment-index="4" -->

--
## Information Retrieval: Vector Space Model
<img src="img/vectorspace.jpg" height="550px">
<br/>
<small>(Source: Wikipedia, By Riclas - Own work, CC BY 3.0, <br/>https://commons.wikimedia.org/w/index.php?curid=9076846)</small>



--
## Words in vector space
<img src="img/figure-1_similarity2.png" height="550px">
<br/>
(artificial data)


--
## Example: French Wikipedia Model
<br/>
* 1.8 million articles, 750 million words
* transform term-document-matrix into dense matrix
* "low-dimensional", dense representation
* skip-gram model, 300 dimensions 
* vector semantics: geometric relations = semantic relations




--
## Similar Words Query
<br/>
```
Query:   ['poésie_nom', 10]
Result:  poétique_adj     0.841
         poème_nom        0.790
         prose_nom        0.733
         littérature_nom  0.715
         poète_nom        0.704
         poétique_nom     0.701
         poésie_nam       0.700
         anthologie_nom   0.695
         littéraire_adj   0.655
         sonnet_nom       0.651
```
<br/>
(authentic data, Wikipedia model)

--
## Similarity Query
<br/>
```
Query: ['prose_nom', 'littérature_nom']
Result: 0.511518681366

Query: ['poésie_nom', 'littérature_nom']
Result: 0.714615326722
```
<br/>
(authentic data, Wikipedia model)

--
## Evaluation
<br/>
* Method: Using a "find-the-wrong word"-task <!-- .element: class="fragment" data-fragment-index="1" -->
* Lists of similar words: <!-- .element: class="fragment" data-fragment-index="2" -->
    * vert, bleu, jaune, rouge, orange
    * billet, monnaie, portemonnaie, payement
* Generate lists with an error <!-- .element: class="fragment" data-fragment-index="3" -->
    * vert, bleu, monnaie, jaune, rouge  
* Wikipedia model: 90% accuracy in finding the error <!-- .element: class="fragment" data-fragment-index="4" -->


--
## Axes of meaning
<img src="img/axes-of-meaning2.png" height="550px">
<br/>
(artifcial data)

--
## Axes of meaning (Ryan Heuser)
<img src="img/fig-abstract-2.jpg" height="500px">
<br/>
(concrete vs. abstract)


--
## Axis query
<br/>
```
Axis: [["bonheur", "joie"],          # positive
       ["malheur", "tristesse"]]     # negative

Query:   ange
Result:  0.0875

Query:   monstre
Result   -0.1407
```
<br/>
(authentic data)



--
## Time for questions!

--
## References

<small>

* Goldberg, Yoav, und Omer Levy. „word2vec Explained: deriving Mikolov et al.’s negative-sampling word-embedding method“. arXiv.org, 2014. http://arxiv.org/abs/1402.3722.
* Heuser, Ryan. „Word Vectors in the Eighteenth Century“. In Digital Humanities 2017: Conference Abstracts, 256–60. Montréal: McGill University & Université de Montréal, 2017.
* Mikolov, Tomas, Kai Chen, Greg Corrado, und Jeffrey Dean. „Efficient Estimation of Word Representations in Vector Space“. arXiv.org, 2013. http://arxiv.org/abs/1301.3781.
* Pennington, Jeffrey, Richard Socher, und Christopher D. Manning. „Glove: Global vectors for word representation“, 2014. doi:10.1.1.671.1743.
* Turney, Peter T., und Patrick Pantel. „From Frequency to Meaning: Vector Space Models of Semantics“. Journal of Artificial Intelligence Research 37 (2010): 141–88. https://arxiv.org/abs/1003.1141.
* Widdows, Dominic. *Geometry and meaning*. CSLI lecture notes, no. 172. Stanford CA: CSLI Publications, 2004.

</small>


--
## Bonus slides

--
## CBOW Model
<a href="img/cbow-model.png"><img src="img/cbow-model.png" height="600px"></a>

--
## Projection
<a href="img/tsne-plot_topn-nom-5800_some-labels.svg"><img src="img/tsne-plot_topn-nom-5800_some-labels.svg" height="600px"></a>

--
## Relation of words in semantic dimensions
<a href="img/roman20_words-2dim_nom_labels.svg"><img src="img/roman20_words-2dim_nom_labels.svg" height="600px"></a>

--
## Comparing models (novels vs. Wikipedia)
<a href="img/figure-6_sens17.png"><img src="img/figure-6_sens17.png" height="600px"></a>



---
# What is Topic Modeling?

--
## (a) Some fundamentals


--
## Topic Modeling: basic idea
<br/>
* Works on the basis of (large) collections of documents <!-- .element: class="fragment" data-fragment-index="1" -->
* Each document is understood as a mixture of topics <!-- .element: class="fragment" data-fragment-index="2" -->
* The purpose is to discover thematic trends and patterns <!-- .element: class="fragment" data-fragment-index="3" -->
* Discovered through generative probabilistic modeling <!-- .element: class="fragment" data-fragment-index="4" -->


--
## Usage scenarios
<br/>
* Information Retrieval: Search not for individual terms, but themes / semantic fields <!-- .element: class="fragment" data-fragment-index="1" -->
* Recommender Systems: Recommend similar journal articles etc. to users <!-- .element: class="fragment" data-fragment-index="2" -->
* Exploration of text collections: what is an email or newspaper corpus about? <!-- .element: class="fragment" data-fragment-index="3" -->
* Research questions from literary studies, cultural studies, history of ideas: topics across authors, genres, time periods <!-- .element: class="fragment" data-fragment-index="4" -->

--
## Explorative Visualization
<p><a href="http://signsat40.signsjournal.org/topic-model/#/model/grid"><img height="500" data-src="img/signs-at-forty.png"></img></a></p>
<p>Signs at 40</p>


--
## Existing Studies
<br/>
* Cameron Blevins: "Topic Modeling Martha Ballard's Diary" (2010): diary <!-- .element: class="fragment" data-fragment-index="1" -->
* Ted Underwood und Andrew Goldstone (2012): "What can topic models of PMLA teach us...": history of a discipline <!-- .element: class="fragment" data-fragment-index="2" -->
* Lisa Rhody, "Topic Modeling and Figurative Language" (2012): ekphrasis in poetry	 <!-- .element: class="fragment" data-fragment-index="3" -->
* Matthew Jockers, Macroanalysis (2013): novel, nationality, gender <!-- .element: class="fragment" data-fragment-index="4" -->
* Ben Schmidt: "Typical TV episodes" (2014): TV shows; temporal development <!-- .element: class="fragment" data-fragment-index="5" -->
* Christof Schöch, "Topic Modeling Genre" (2017): drama, subgenres <!-- .element: class="fragment" data-fragment-index="6" -->


--
## (b) A topic model for French crime fiction

--
## Text collection: 840 French Novels
<img height="500" data-src="img/textsammlung.png"></img>

--
## Crime fiction (prototypical)
<br/>
* Long, narrative, fictional prose (=novel) <!-- .element: class="fragment" data-fragment-index="1" -->
* Character inventory: investigators, criminals, suspects, witnesses, victims <!-- .element: class="fragment" data-fragment-index="2" -->
* Plot: violent crime, rational elucidation <!-- .element: class="fragment" data-fragment-index="3" -->
* Setting: urban space <!-- .element: class="fragment" data-fragment-index="4" -->
* => Hypotheses regarding possible topics <!-- .element: class="fragment" data-fragment-index="5" -->

--
## Topic and subgenre
<p><a href="img/2_topic10-wordle-comparison.png"><img height="400" data-src="img/2_topic10-wordle-comparison.png"></img></a></p>
<p><b>Topic 10: detective, inspector, police</b></p>
<p>Distinctive of crime fiction (content & statistics) (p &lt; α=0.01)</p> <!-- .element: class="fragment" data-fragment-index="1" -->


--
##Topic and subgenre
<p><a href="img/2_topic49-wordle-comparison.png"><img height="400" data-src="img/2_topic49-wordle-comparison.png"></img></a></p>
<p><b>Topic 49: death, crime, to kill</b></p>
<p>Distinctive of crime fiction (content & statistics) (p &lt; α=0.01)</p>


--
##Topic and subgenre
<p><a href="img/2_topic47-wordle-comparison.png"><img height="400" data-src="img/2_topic47-wordle-comparison.png"></img></a></p>
<p><b>Topic 47: door, room, to open</b></p>
<p>Statistically distinctive (p &lt; α=0.01); but content-wise?</p> 


--
##Topic and subgenre
<p><a href="img/2_topic26-wordle-comparison.png"><img height="400" data-src="img/2_topic26-wordle-comparison.png"></img></a></p>
<p><b>Topic 26: beach, sand, sun</b></p>
<p>Distinctive of non-crime fiction (p &lt; α=0.001)</p>


--
## Topics over text segments
<p><a href="img/2_topic002_progression.png"><img height="400" data-src="img/2_topic002_progression.png"></img></a></p>
<p><b>Topic 2: judge, prison, lawyer/attorney</b></p>
<p>Statistically significant (crime fiction): (1,4), (4,5) etc.</p>


--
## Topics over text segments
<p><a href="img/2_topic033_progression.png"><img height="400" data-src="img/2_topic033_progression.png"></img></a></p>
<p><b>Topic 33: black, hair, eyes, wear, eye, face</b></p>
<p>Statistically significant: crime fiction all but (2,3);<br/> non-crime fiction (1,3), (2,5)</p>

--
## Overall results
<br/>
* A large part of the topics is statistically distinctive: crime fiction (31/80) non-crime fiction (21/80) <!-- .element: class="fragment" data-fragment-index="1" -->
* Topics are not just themes, but also narrative motives, descriptive elements, character sets <!-- .element: class="fragment" data-fragment-index="2" -->
* Textual progression: only a few topics have significant trends <!-- .element: class="fragment" data-fragment-index="3" -->
* Overall: we can detect thematic trends in 840 novels without reading (all of) them! <!-- .element: class="fragment" data-fragment-index="4" -->


--
## Time for questions


--
## Bonus slides: visualizations

--
## Topics and subgenres: topic 3
<img height="500" data-src="img/tI_by-subsubgenre-003.png"></img>

--
## Topics and authors: topic 3
<img height="500" data-src="img/tI_by-author-name-003.png"></img>

--
## Topics / subgenres heatmap
<img height="500" data-src="img/dist-heatmap_by-subsubgenre_policier-smallset.jpg"></img>

--
## topic clustering
<img height="500" data-src="img/topic-clustering_cosine-weighted-50words.png"></img>
<p>(top 50 topics, cosine/weighted)</p>

--
## topic clustering (detail)
<img height="500" data-src="img/topic-clustering_cosine-weighted-50words_3-227.png"></img>
<p>(top 50 topics, cosine/weighted)</p>

--
## Topics over decades
<img height="500" data-src="img/lineplot-3-227.png"></img>

--
## Topics and authors: clustering
<img height="500" data-src="img/item-clustering_author-name_cosine-weighted-mean-50topics.jpg"></img>

--
## topic-work bimodal network
<img height="500" data-src="img/author-topic-map_full.jpg"></img>

--
## topic-work bimodal network (detail)
<img height="500" data-src="img/author-topic-map_sub-neopolar.jpg"></img>


--
## topics over text progression
<img height="500" data-src="img/topics_fallend.png"></img>

--
## topics over text progression
<img height="500" data-src="img/topics_ansteigend.png"></img>

--
## topics by genre and text progression
<img height="500" data-src="img/textverlauf-untergattung.png"></img>

--
## PCA based on topic scores (subgenres)
<a href="img/topic-pca.png"><img height="500" data-src="img/topic-pca.png"></a></img>





---
# Topic Modeling: Theory

--
## (a) How does a topic model look like?

--
## On a practical level
<br/>
* A topic is a group of words with some (semantic) relation (e.g., common theme, motive, etc.) <!-- .element: class="fragment" data-fragment-index="1" -->
* Each topic is made up of words of varying importance and relevance to the topic <!-- .element: class="fragment" data-fragment-index="2" -->
* Each document is made up of several topics in various proportions <!-- .element: class="fragment" data-fragment-index="3" -->

--
## On a technical level
<br/>
* A topic model is an abstract representation of all topics and documents in a collection <!-- .element: class="fragment" data-fragment-index="1" -->
* A topic is a probability distribution over words <!-- .element: class="fragment" data-fragment-index="3" -->
* A document is a probability distribution over topics <!-- .element: class="fragment" data-fragment-index="4" -->
* The Dirichlet distribution (in LDA) describes the topic mixture distribution of the model <!-- .element: class="fragment" data-fragment-index="5" -->

--
## Words in topic distribution
<p><img height="400" data-src="img/word-scores-in-topics.png"></img></p>
<p>(Each word has a score in each topic; here ordered by topic/rank)

--
## Topics in document distribution
<p><img height="400" data-src="img/docs-and-topics-in-rank.png"></img></p>
<p>(Each topic has a score in each document; ordered by document)



--
## (b) How is a Topic Model created?


--
## Some relevant ideas
<br/>
* The most widespread implementation uses 'Latent Dirichlet Allocation'
* Follows the "bag-of-words"-model: word order is irrelevant <!-- .element: class="fragment" data-fragment-index="1" -->
* No semantic knowledge / dictionary / WordNet etc. is used; language-independent <!-- .element: class="fragment" data-fragment-index="2" -->
* Based on distributional semantics: "a word is characterized by the company it keeps" (John Firth 1957) <!-- .element: class="fragment" data-fragment-index="3" -->
* Discovers words which frequently occur together or in similar contexts (=topics)  <!-- .element: class="fragment" data-fragment-index="4" -->
* Infers how important each word is in each topic <!-- .element: class="fragment" data-fragment-index="5" -->
* Infers how important each topic is in each document <!-- .element: class="fragment" data-fragment-index="6" -->


--
## Generative, inverted, iterative

>"A topic model is a generative model for documents: it specifies a simple probabilistic procedure by which documents can be generated. To make a new document, one chooses a distribution over topics. Then, for each word in that document, one chooses a topic at random according to this distribution, and draws a word from that topic. Standard statistical techniques can be used to invert this process, inferring the set of topics that were responsible for generating a collection of documents." <br/>(Steyvers and Griffiths 2006)


--
## Inference problem: observed data
<p><img height="500" data-src="img/blei_topic-modeling_observed.png"></img></p>
<p>(David Blei, "Topic Models" lecture, 2009)</p>

--
## Inferred, latent model
<p><img height="500" data-src="img/tm_blei.png"></img></p>
<p>(David Blei, "Probabilistic Topic Models", 2012)</p>


--
## Bayesian Statistics
>"The computational problem of inferring the hidden topic structure from the documents is the problem of computing the posterior distribution, the conditional distribution of the hidden variables, given the documents."

<br/>
<p>(David Blei, "Probabilistic Topic Models", 2012)</p>


--
## Inference task
<br/>
###p(Z, φ, θ | w, α, β)
<br/>
<small>

* Compute the probability p of the latent variables... 
	* Z = assignments of each word in each document to a topic
	* φ (phi) = distribution over words (for each topic)
	* θ (theta) = distribution over topics (for each document)
* ...given our observed variables (input data) and parameters
	* w = the data, i.e. the words in each document
	* α = parameter of the Dirichlet prior for topics per document
	* β = parameter of the Dirichlet prior for words per topic

</small>


--
## Dirichlet distributions
<p><a href="img/dirichlet_plot.png"><img height="500" data-src="img/dirichlet_plot.png"></img></a></p>
<p>(Describe the topic mixture distributions of the model)<br/>(Here several possible distributions with three topics)</p>


--
## The starting point of LDA
<br/>
* We have the documents with their words (e.g. as a word/document frequency matrix)  <!-- .element: class="fragment" data-fragment-index="1" -->
* We are looking for the word distributions per topic, the topic distributions per document, and the topic assignment of each word  <!-- .element: class="fragment" data-fragment-index="2" -->
* Both distributions are dependent on each other (if a topic changes, the topic distributions change)  <!-- .element: class="fragment" data-fragment-index="3" -->
* And both distributions need to fit with the original documents  <!-- .element: class="fragment" data-fragment-index="4" -->


--
## The generative model behind LDA
<br/>
* For each topic, there is a distribution over words <!-- .element: class="fragment" data-fragment-index="1" -->
* For each document, there is a distribution over topics <!-- .element: class="fragment" data-fragment-index="2" -->
* For each word in each document: <!-- .element: class="fragment" data-fragment-index="3" -->
    * We sample a topic from the topic distribution of that document
    * We sample a word from the word distribution of that topic
* This can only work if we have the distributions; which we don't <!-- .element: class="fragment" data-fragment-index="4" -->

--
## Random initialization
<br/>
* For each document, we generate a random distribution over topics <!-- .element: class="fragment" data-fragment-index="1" -->
* For each topic, we generate a random distribution over words <!-- .element: class="fragment" data-fragment-index="2" -->
* For each word in each document:  <!-- .element: class="fragment" data-fragment-index="3" -->
    * Sample a topic from the topic distribution
    * Sample a word from the word distribution of that topic
* Now we have a model; but we know it's most likely wrong (=low confidence) <!-- .element: class="fragment" data-fragment-index="4" -->

--
## Inference: iterative approximation
<br/>
* Using the observed data and our (random/erroneous) model, we can improve the model <!-- .element: class="fragment" data-fragment-index="1" -->
* One among several methods: Gibbs sampling <!-- .element: class="fragment" data-fragment-index="2" -->
	* For one word in one document, remove the existing topic assignment <!-- .element: class="fragment" data-fragment-index="3" -->
    * Based on the model and the other words in the document, assign a new topic to the word (cooccurrence!) <!-- .element: class="fragment" data-fragment-index="4" -->
    * Update the overall model according to this assignment;  <!-- .element: class="fragment" data-fragment-index="5" -->
* Repeat until your time runs out or your evaluation task says it's ok to stop <!-- .element: class="fragment" data-fragment-index="6" -->

--
## How it works exactly, clearly explained
<p><img height="600" data-src="img/sharris_more-explicit-in-step-two.jpg"></img></p>  <!-- .element: class="fragment" data-fragment-index="1" -->



--
## Time for questions

--
## Further Reading: Theory
<small>

**Introductory articles**
* Blei, David M. (2012). "Probabilistic topic models". In: _Communications of the ACM_, 55(4): 77–84. <http://www.cs.princeton.edu/~blei/papers/Blei2012.pdf>
* Steyvers, M. and Griffiths, T. (2006). "Probabilistic Topic Models". In: Landauer, T. et al. (eds), _Latent Semantic Analysis: A Road to Meaning_. Laurence Erlbaum.

**Video lectures**
* Jordan Boyd-Graber, "Topic Models", YouTube.com, 2015. <https://www.youtube.com/watch?v=yK7nN3FcgUs>
* David Blei, "Topic Models", Videolectures.net, 2012, <http://videolectures.net/mlss09uk_blei_tm/>

</small>



--
## Bonus slides

--
## Topic modeling: geometric interpretation
<p><img height="500" data-src="img/steyvers_geometric-interpretation-of-the-topic-model.png"></img></p>
<p>(Steyvers and Griffiths, "Probabilistic Topic Modeling", 2006)</p>


--
## Latent Dirichlet Allocation: plate notation
<p><img height="500" data-src="img/steyvers_topic-modeling-plate-notation.png"></img></p>
<p>(<https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>)</p>


--
## Latent Dirichlet Allocation: plate notation
<p><img height="400" data-src="img/wikipedia_topic-modeling-plate-notation_legend.png"></img></p>
<p>(<https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>)</p>



--
## Latent Dirichlet Allocation: plate notation
<small>

* D = number of documents (given)
* T = number of topics (set by researcher)
* Nd = number of words in document d
* α (alpha): Dirichlet prior (hyperparameter: sparse / smooth distribution of topics)
* β (beta): Dirichlet prior (hyperparameter: sparse / smooth distribution of words)
* θ (theta): distribution over topics (for each document; latent variable)
* ϕ (phi): distribution over words (for each topic; latent variable) 
* z = assignment of words to topics (latent variable)
* w = words in a document (observed variable)

</small>






---
# A Topic Modeling pipeline


--
## A Topic Modeling pipeline
<p><a href="img/tm-workflow_eng.png"><img height="500" data-src="img/tm-workflow_eng.png"></img></a></p>


--
## Some parameters
<br/>
* Preprocessing: text segmentation, lemmatization, feature selection <!-- .element: class="fragment" data-fragment-index="1" -->
* Modeling: number of topics, number of iterations, etc.  <!-- .element: class="fragment" data-fragment-index="2" -->
* Evaluation: model quality measure  <!-- .element: class="fragment" data-fragment-index="3" -->
* Postprocessing: level of metadata / text linkage  <!-- .element: class="fragment" data-fragment-index="4" -->
* Visualization: many options  <!-- .element: class="fragment" data-fragment-index="5" -->

--
## The pipeline in Python
<br/>
* Preprocessing: NLTK / TextBlob  <!-- .element: class="fragment" data-fragment-index="1" -->
* Corpus ingest, modeling, evaluation: gensim  <!-- .element: class="fragment" data-fragment-index="2" -->
* Postprocessing: pandas  <!-- .element: class="fragment" data-fragment-index="3" -->
* Visualisation: pyLDAvis, seaborn, wordcloud, etc.  <!-- .element: class="fragment" data-fragment-index="4" -->

--
## Time for questions

--
## Tutorials
<br/>
* Weingart, Scott (2012). "Topic Modeling for Humanists: A Guided Tour". In: _The Scottbot Irregular_. <http://www.scottbot.net/HIAL/?p=19113>
* Graham, Shawn, Scott Weingart and Ian Milligan (2012). "Getting Started with Topic Modeling and MALLET". _The Programming Historian_. <http://programminghistorian.org/lessons/topic-modeling-and-mallet>
* Riddell, Allen. (2014). "TAToM: Text Analysis with Topic Modeling for Humanities Scholars". In: _DARIAH-DE Schulungsmaterialien_. <https://de.dariah.eu/tatom/>







---
# First steps doing Topic Modeling


--
## Some starting points
<br/>
* This section of the slides: <br/>https://christofs.github.io/riga/#/9 
* Dataset and scripts package: <br/>https://www.dropbox.com/s/orq3nx0zmui96l9/tm-downloads.zip?dl=0
* Also publicly available at: <br/>https://github.com/dh-trier/topicmodeling/

--
## Getting ready
<br/>
* Has everyone got the "OK"s when running the test script?
* Please download and unzip the dataset linked above
* So let's all launch Thonny 
* Install additional libraries: seaborn, wordcloud

--
## The workshop data
<br/>
* datasets/
* results/
* scripts/

--
## The script architecture
<br/>
* each step in the pipeline (input-output) is one module
* each module consists of several functions
* a "main" function coordinates these functions
* the "run_pipeline.py" script coordinates the modules
* NB: each module reads and writes data

--
## A closer look at "run_pipeline1.py"
<br/>
* Imports
* Files and Folders
* Parameters
* Functions
* Coordinating function

--
## Step by step: preprocessing
<br/>
* Open "preprocessing.py" with Thonny
* Note the parameters
* Note the file structure
* Note the flow of the data (input/output)
* Run it from "run_pipeline1.py"


--
## Running the pipeline one by one
<br/>
* preprocessing
* build_corpus
* modeling
* postprocessing
* make_overview 


--
## Practice

--
## Exercise 1: run "run_pipeline1.py"
<br/>
* Use the small "hkpress-test" dataset
* Decide on your own parameters
* Run the entire pipeline (step by step or in one go)
* What error messages do you get, if any?
* What kind of results do you get?

--
## Exercise 2: Adapt the commands
<br/>
* Continue using the "hkpress-test" corpus
* Decide on a new "identifier" for your model
* At your choice, do *one* of the following 
    * Modify the stopword list (in: preprocessing.py)
    * Use a different number of topics (in: run_pipeline1.py)
* Inspect the results and write down any changes you notice




---
#  More issues in Topic Modeling

--
## Activity 1: More visualisations 
<br/>
* Wordles! module "make_wordle" (in "run_pipeline2.py")
* Topic probability distribution heatmaps ("make_heatmaps");<br/>depends on "metadata.csv"


--
## Activity 2: Model evaluation 
<br/>
* "evaluation.py" (in "run_pipeline2.py")
    * overall model coherence (best: c_v)
    * individual topic coherence (c_v)
* various measures of model quality
* many types of evaluations (beyond the code here)

--
## Topic Coherence Measures 
<a href="img/coherence-measures.png"><img height="500" data-src="img/coherence-measures.png"></img></a>
<br/><small>(Röder et al., "Exploring the Space of Topic Coherence Measures", 2015)</small>


--
## Activity 3 / Discussion: bring your own corpus
<br/>
* If there is time left... 
* Does anyone have a collection of many short English-language texts?
* How would you go about to run a model for it?
* What if your text collection is neither English nor French? 


---
# Wrapping up


--
## Summary of what we have covered
<br/>
* A bit of background on distributional semantics
* An idea of what a topic model consists of <!-- .element: class="fragment" data-fragment-index="1" -->
* An intuition of how topic models are inferred <!-- .element: class="fragment" data-fragment-index="2" -->
* Some avenues for interpreting and visualizing topic models <!-- .element: class="fragment" data-fragment-index="3" -->
* The overall workflow required for topic modeling <!-- .element: class="fragment" data-fragment-index="4" -->
* How to use Python for topic modeling <!-- .element: class="fragment" data-fragment-index="5" -->

--
## A few things not covered here
<br/>
* Implementation details of Gibbs Sampling <!-- .element: class="fragment" data-fragment-index="2" -->
* Precursors of LDA: LSA, pLSA, NNMF, etc. <!-- .element: class="fragment" data-fragment-index="3" -->
* Variants of LDA: hierarchical, labeled, dynamic, etc. <!-- .element: class="fragment" data-fragment-index="4" -->
* Evaluation strategies: human evaluation, external, internal  <!-- .element: class="fragment" data-fragment-index="5" -->


--
## Your questions and projects
<br/>
* What kind of projects / text collections do you have?
* What kind of research questions do you have?
* What do you think topic modeling could tell you?
* ...



</section>


--
<br/>
<br/>
<br/>
## Thank you! | Paldies!
<br/>
<br/>
<br/>
<br/>
<br/>
<hr/>
<p>Christof Schöch, 2019</p>
<p><a href="https://christofs.github.io/">christofs.github.io/riga</a></p>
<p><a href="https://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a><br/></p>
<hr/>
<br/>
<br/>


</script>
</section>



<!-- DON'T TOUCH UNLESS YOU KNOW WHAT YOU'RE DOING :-) -->
</div>
<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>
<script>
// Full list of configuration options available at:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
    controls: true,
    progress: true,
    slideNumber: true,
    history: true,
    center: true,
    transition: 'slide', // none/fade/slide/convex/concave/zoom
    // Optional reveal.js plugins
    dependencies: [
        { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
        { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
        { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
        { src: 'plugin/zoom-js/zoom.js', async: true },
        { src: 'plugin/notes/notes.js', async: true }
        ]
    });
</script>
</body>
</html>
